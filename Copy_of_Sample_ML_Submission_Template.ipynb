{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "BZR9WyysphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "4_0_7-oCpUZd",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "id1riN9m0vUs",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "dJ2tPlVmpsJ0",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarmanikpuri/Netflix-Machine-Learning/blob/main/Copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -  Netflix Machine Learning\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Exploratory Data Analysis & unsupervised Machine Learning\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project analyzes a dataset of Netflix movies and TV shows available as of 2019, collected from Flixable, a third-party Netflix search engine. The objective is to perform Exploratory Data Analysis (EDA), understand content distribution across countries, evaluate Netflix’s shift in focus from movies to TV shows, and apply unsupervised learning to cluster similar titles.\n",
        "\n",
        "The project begins with cleaning and preparing the dataset, handling missing values, splitting multiple countries, fixing date formats, and standardizing ratings. This ensures the accuracy and consistency of all insights generated.\n",
        "\n",
        "The EDA highlights several important patterns. Netflix hosts more movies than TV shows overall, but a year-wise trend analysis shows that TV show additions have grown significantly in recent years, while movie additions have gradually declined. This confirms that Netflix has increasingly focused on producing and acquiring TV shows, aligning with industry reports of shifting viewer preferences toward series-style content.\n",
        "\n",
        "Country-wise analysis reveals that the United States contributes the largest share of Netflix titles, followed by countries such as India, the United Kingdom, and Japan. The dataset also shows clear variation in genre preferences across regions. For example, Indian titles are dominated by drama and romance categories, while U.S. content includes a higher proportion of documentaries and TV shows. This emphasizes Netflix’s global content strategy and its efforts to offer region-specific content.\n",
        "\n",
        "The final component of the project involves clustering similar titles using unsupervised learning. By transforming text features such as descriptions and genres into numerical representations (using techniques like TF-IDF) and applying clustering algorithms like KMeans, the project groups shows and movies based on similarity. These clusters represent patterns such as crime thrillers, family-friendly content, romantic dramas, or documentaries. Such clustering is useful for content recommendations and understanding how different types of shows and movies relate to each other.\n",
        "\n",
        "Overall, this project provides meaningful insights into Netflix’s content library, its global distribution, and strategic trends. It also demonstrates the use of EDA and unsupervised machine learning techniques to derive actionable findings from real-world entertainment data."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Perform Exploratory Data Analysis (EDA) to understand the structure, composition, and key characteristics of Netflix titles.\n",
        "\n",
        "2.Examine the type of content available across different countries and identify regional differences in genre, format, and production.\n",
        "\n",
        "3.Investigate whether Netflix has been increasingly focusing on TV shows rather than movies in recent years by analyzing year-wise content addition trends.\n",
        "\n",
        "4.Apply unsupervised learning techniques to cluster similar content by using text-based features such as descriptions and genres."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file path\n",
        "file_path = '/content/drive/My Drive/Netflix Machine Learning/NETFLIX MOVIES & TV SHOWS.xlsx'\n",
        "df = pd.read_excel(file_path)"
      ],
      "metadata": {
        "id": "DQd1qjvmXkhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "duplicates_count = df.duplicated().sum()\n",
        "duplicates_count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "missing = df.isnull().sum()\n",
        "missing = missing[missing > 0]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "missing.plot(kind='bar')\n",
        "plt.title('Missing Values Count per Column')\n",
        "plt.xlabel('Columns')\n",
        "plt.ylabel('Missing Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#columns names\n",
        "string_colimns = [\"director\", \"cast\", \"country\", \"rating\"]\n",
        "date_col = ['date_added']\n",
        "\n",
        "#Replace missing values in string columns\n",
        "df[string_colimns] = df[string_colimns].fillna(\"Not provided\")\n",
        "\n",
        "#Replace missing values in date_column\n",
        "df[date_col] = df[date_col].fillna(pd.Timestamp.today())\n",
        "\n",
        "#count Missing / Null values after replace\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "pY98oM8wYXxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"The dataset contains 7787 Rows and 12 Columns\" \"It include mixed of string, numeric, and date columns\"\n",
        "\n",
        "\"Columns include details like show_id, type, title, director etc, and date column called date_added\"\n",
        "\n",
        "\"Numeric fields include release_year\"\n",
        "\n",
        "\"I checked duplicates and found \"0\" duplicates, so no removal was needed\"\n",
        "\n",
        "\"four string columns had missing values, which i replaced with \"Not provided\"\"\n",
        "\n",
        "\"The date_added column had missing values which i filled with earliest available date\"Answer Here"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check uniques values\n",
        "for column in df.columns:\n",
        "    print(f\"Column: {column}\")\n",
        "    print(f\"Unique count:\", df[column].unique())\n",
        "    print(\"Unique Values:\", df[column].unique())\n",
        "    print(\"--------------\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "SGsuVFhmYsH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "show_id Data Type: String / Object\n",
        "Description: A unique identifier assigned to each show or movie.\n",
        "\n",
        "Example Values: \"s1\", \"m35\"\n",
        "\n",
        "Notes: Used to uniquely identify each record.\n",
        "\n",
        "type Data Type: String (Categorical)\n",
        "\n",
        "Description: Indicates whether the content is a Movie or a TV Show.\n",
        "\n",
        "Example Values: \"Movie\", \"TV Show\"\n",
        "\n",
        "title Data Type: String\n",
        "\n",
        "Description: Name of the show or movie.\n",
        "\n",
        "Example Values: \"Stranger Things\", \"Inception\"\n",
        "\n",
        "director Data Type: String\n",
        "\n",
        "Description: Name of the director(s) of the show or movie.\n",
        "\n",
        "Example Values: \"Christopher Nolan\", \"N/A\"\n",
        "\n",
        "Notes: Often contains missing values.\n",
        "\n",
        "cast Data Type: String\n",
        "\n",
        "Description: Names of actors involved in the show or movie.\n",
        "\n",
        "Example Values: \"Robert Downey Jr., Chris Evans\"\n",
        "\n",
        "Notes: Can contain missing values (especially for documentaries or stand-up shows).\n",
        "\n",
        "country Data Type: String\n",
        "\n",
        "Description: Country where the movie/show was produced.\n",
        "\n",
        "Example Values: \"United States\", \"India\"\n",
        "\n",
        "Notes: May contain multiple countries.\n",
        "\n",
        "date_added Data Type: Date / Datetime\n",
        "\n",
        "Description: The date when the movie or show was added to the platform (e.g., Netflix).\n",
        "\n",
        "Example Values: \"2019-07-10\", \"2020-01-01\"\n",
        "\n",
        "Notes: Missing values are common.\n",
        "\n",
        "release_year Data Type: Integer\n",
        "\n",
        "Description: The year when the movie/show was originally released.\n",
        "\n",
        "Example Values: 2017, 2020\n",
        "\n",
        "rating Data Type: String (Categorical)\n",
        "\n",
        "Description: Age rating or maturity level assigned to the content.\n",
        "\n",
        "Example Values: \"PG-13\", \"TV-MA\", \"R\"\n",
        "\n",
        "duration Data Type: String\n",
        "\n",
        "Description: Duration of the movie or number of seasons for TV shows.\n",
        "\n",
        "Example Values: \"90 min\", \"2 Seasons\"\n",
        "\n",
        "Notes: Contains mixed text values (minutes & seasons).\n",
        "\n",
        "listed_in Data Type: String\n",
        "\n",
        "Description: Genre or category of the content.\n",
        "\n",
        "Example Values: \"Drama\", \"Comedy\", \"Action & Adventure\"\n",
        "\n",
        "Notes: Often includes multiple genres.\n",
        "\n",
        "description Data Type: String\n",
        "\n",
        "Description: A short summary or description of the show or movie.\n",
        "\n",
        "Example Values: \"A young boy disappears...\", \"A detective solves...\""
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "#1.check dataset structures\n",
        "print(df.shape)\n",
        "print(df.dtypes)\n",
        "print(df.columns)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correct data types\n",
        "#convert date_added to date time\n",
        "df[\"date_added\"] = pd.to_datetime(df[\"date_added\"], errors = 'coerce')\n",
        "\n",
        "#clean duration into numeric value + unit\n",
        "df[\"duration_value\"] = df[\"duration\"].str.extract(\"(\\d+)\").astype(\"float\")\n",
        "df[\"duration_unit\"] = df[\"duration\"].str.extract(\"([A-Za-z]+)\").astype(str)\n",
        "\n",
        "# Optional: convert release_year to int (already correct)\n",
        "df[\"release_year\"] = df[\"release_year\"].astype(int)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Sb2J2UyPZP7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Clean text columns\n",
        "# -------------------------------\n",
        "string_cols = [\"type\", \"title\", \"director\", \"cast\", \"country\",\n",
        "               \"rating\", \"duration\", \"listed_in\", \"description\"]\n",
        "\n",
        "# Strip extra spaces\n",
        "for col in string_cols:\n",
        "    df[col] = df[col].astype(str).str.strip()\n",
        "\n",
        "# Standardize formatting\n",
        "df[\"type\"] = df[\"type\"].str.title()\n",
        "df[\"country\"] = df[\"country\"].str.title()"
      ],
      "metadata": {
        "id": "N2x-CDgMZQqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Final Summary\n",
        "# -------------------------------\n",
        "print(\"\\nFinal Dataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nPreview:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NrjmeIdVZTDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "type_counts = df['type'].value_counts()\n",
        "\n",
        "plt.figure()\n",
        "plt.pie(type_counts, labels=type_counts.index, autopct='%1.1f%%')\n",
        "plt.title('Type Distribution')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand the proportion of Movies vs TV Shows in the dataset.Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Movies dominate the platform with 69.1%, while TV Shows account for 30.1%.Answer Here."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps in identifying content imbalance and avoiding bias in recommendation or classification models.Answer Here."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Convert Date_added to Datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "#Group data by release year\n",
        "year_count = df.groupby('release_year').size()\n",
        "\n",
        "#create the line chart\n",
        "plt.figure()\n",
        "plt.plot(year_count.index, year_count.values, marker='o')\n",
        "plt.xlabel('Release Year')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.title('Number of Title Released Per Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To analyze how the platform’s content additions have changed over time.Answer Here."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The trend shows an increase in content additions in recent years, indicating platform expansion."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps identify growth phases and supports time-based analysis for demand forecasting and recommendation models."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "#Drop Missing Values\n",
        "release_year_data = df['release_year'].dropna()\n",
        "\n",
        "# Create Histogram\n",
        "plt.hist(release_year_data, bins=20)\n",
        "plt.xlabel('Release year')\n",
        "plt.ylabel('Number of Movies')\n",
        "plt.title('Distribution of Movies by Release Year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen because it is the most suitable chart to visualize the distribution of movies by release year. Since release_year is a numerical and continuous variable, a histogram effectively shows how movies are spread across different time periods and helps identify trends and peak release years."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the histogram analysis, it is observed that 2020 is the dominating year, With the highest number of movies and tv shows released by Netflix. This indicates a peak in content additions during that year."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights create a positive business impact by identifying peak release years, which helps in better content planning and engagement.\n",
        "\n",
        "However, over-concentration in a single year (2020) may indicate uneven content distribution, posing a risk to consistent long-term growth."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Count Movies/TV shows by country(Drop Missing Values)\n",
        "country_counts = df['country'].dropna().value_counts().head(10)\n",
        "\n",
        "#create horizontal bar chart\n",
        "plt.barh(country_counts.index, country_counts.values)\n",
        "plt.xlabel('Number of Movies/TV shows')\n",
        "plt.ylabel('Country')\n",
        "plt.title('Top 10 Countries By content on netflix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A horizontal bar chart was chosen because it is suitable for visualizing the top countries producing content on Netflix, as it clearly compares categorical data and improves readability for country names."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "USA leads in Netflix content production, followed by India and the UK."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insights create a positive business impact by helping Netflix focus on top content-producing countries like the USA, India, and the UK.\n",
        "\n",
        "However, over-dependence on a few countries may limit regional diversity and affect growth in other markets."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "from collections import Counter\n",
        "\n",
        "# Drop missing values\n",
        "listed_in_data = df['listed_in'].dropna()\n",
        "\n",
        "# Split genres and flatten the list\n",
        "all_genres = []\n",
        "for genres in listed_in_data:\n",
        "    all_genres.extend([g.strip() for g in genres.split(',')])\n",
        "\n",
        "# Count frequency of each genre\n",
        "genre_counts = Counter(all_genres)\n",
        "\n",
        "# Convert to dictionary for plotting\n",
        "top_genres = dict(genre_counts.most_common(10))  # Top 10 genres\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(top_genres.keys(), top_genres.values(), color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.xlabel('Genre')\n",
        "plt.ylabel('Number of Movies / TV Shows')\n",
        "plt.title('Top 10 Genres on Netflix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because it clearly shows and compares the top dominating genres on Netflix."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis shows that International and Drama genres are the most popular and highly watched by viewers on Netflix."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insight has a positive business impact, as Netflix can focus on producing more content in dominating genres. However, the negative aspect is that some genres receive fewer views, so targeted strategies are needed to improve the performance of less-viewed genres."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Count movies/TV shows by rating (drop missing values)\n",
        "rating_counts = df['rating'].dropna().value_counts()\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(rating_counts.index, rating_counts.values)\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Number of Movies / TV Shows')\n",
        "plt.title('Distribution of Content Ratings on Netflix')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because it clearly visualizes and compares the distribution of content ratings."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis shows that TV-MA, TV-14, and TV-PG have the highest number of content on Netflix.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights help create a positive business impact by showing that TV-MA, TV-14, and TV-PG dominate Netflix’s content. This helps Netflix focus content creation and marketing on ratings with the highest audience demand.\n",
        "\n",
        "However, the insights also indicate a potential negative growth risk. Over-concentration on a few ratings may limit content for niche or younger audiences, which could reduce engagement from those segments if not addressed."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Select only Movie durations and drop missing values\n",
        "movie_duration = df[df['duration'].str.contains('min', na=False)]['duration']\n",
        "\n",
        "# Extract minutes as integer\n",
        "movie_duration_minutes = movie_duration.str.replace(' min', '').astype(int)\n",
        "\n",
        "# Create histogram\n",
        "plt.hist(movie_duration_minutes, bins=20)\n",
        "plt.xlabel('Duration (Minutes)')\n",
        "plt.ylabel('Number of Movies')\n",
        "plt.title('Distribution of Movie Durations on Netflix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram was chosen because it effectively shows the distribution of movie and TV show durations on Netflix."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis shows that content with a duration of 85–115 minutes has the highest count, with over 1,400 titles on Netflix."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the insight has a positive business impact, as viewers prefer content with durations between 85–115 minutes. However, content with lower viewership durations requires improvement in quality or presentation to increase engagement."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Count content by director (drop missing values)\n",
        "director_counts = df['director'].dropna().value_counts().head(10)\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(director_counts.index, director_counts.values)\n",
        "plt.xlabel('Director')\n",
        "plt.ylabel('Number of Movies / TV Shows')\n",
        "plt.title('Top 10 Directors by Content on Netflix')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart was chosen because it clearly shows the top directors dominating content on Netflix."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis shows that content with missing or unspecified director names is the most dominant category on Netflix."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights can create a positive business impact by highlighting gaps in metadata quality. Improving director information can enhance content discovery, search accuracy, and recommendation systems on Netflix.\n",
        "\n",
        "However, the insight also indicates a potential negative growth issue. A large amount of content without director details may reduce user trust, limit personalization, and negatively affect viewer engagement if not addressed."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Drop missing values in 'type' and 'rating'\n",
        "data = df[['type', 'rating']].dropna()\n",
        "\n",
        "# Create a cross-tab for stacking\n",
        "type_rating_counts = pd.crosstab(data['type'], data['rating'])\n",
        "\n",
        "# Plot stacked bar chart\n",
        "type_rating_counts.plot(kind='bar', stacked=True, figsize=(10,6))\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Number of Titles')\n",
        "plt.title('Stacked Bar Chart of Type vs Rating on Netflix')\n",
        "plt.xticks(rotation=0)\n",
        "plt.legend(title='Rating', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A stacked bar chart is suitable here to compare how ratings are distributed across different content types (Movie vs TV Show), giving insights into viewer demographics and content strategy."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The analysis indicates that the number of movies on Netflix is higher than TV shows, with over 5,000 movies compared to approximately 2,500 TV shows."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The positive impact is that viewers are more likely to watch movies, as they require less time compared to TV shows. The negative aspect is that Netflix needs to focus on improving the quality of TV shows to increase engagement and attract more viewers."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# Combine all descriptions into one string, dropping missing values\n",
        "text = \" \".join(df['description'].dropna().astype(str))\n",
        "\n",
        "# Create Word Cloud\n",
        "wordcloud = WordCloud(width=800, height=300, background_color='white', colormap='viridis', stopwords=None).generate(text)\n",
        "\n",
        "# Plot Word Cloud\n",
        "plt.figure(figsize=(15,7))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Netflix Descriptions', fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Word Cloud is used to visualize the most common words in Netflix descriptions and highlight popular content themes."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LIFE, LOVE, FAMILY, FRIENDS, NEW, WOMEN, TAKE, and LIVE are the most frequent words in Netflix descriptions, highlighting popular content themes."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive: Shows popular themes, guiding content strategy and engagement.\n",
        "\n",
        "Negative: Over-focusing on frequent themes may neglect niche audiences, limiting growth."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statement 1: Movies are more frequent than TV Shows on Netflix.\n",
        "\n",
        "Statement 2: Content added after 2018 has higher average release year.\n",
        "\n",
        "Statement 3: International movies, dramas, and comedy are top genres in Netflix."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀: Movies ≤ TV Shows on Netflix\n",
        "\n",
        "H₁: Movies > TV Shows on Netflix"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Count Movies and TV Shows\n",
        "type_counts = df['type'].value_counts()\n",
        "\n",
        "# Observed frequencies\n",
        "observed = type_counts.values\n",
        "\n",
        "# Expected frequencies (assuming equal distribution)\n",
        "expected = [observed.sum()/len(observed)] * len(observed)\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi_stat, p_value = chisquare(f_obs=observed, f_exp=expected)\n",
        "\n",
        "chi_stat, p_value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To obtain the p-value, a Chi-Square Test was performed."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square test was used to compare categorical frequencies. Since the p-value was less than 0.05, the null hypothesis was rejected, confirming that Movies dominate TV Shows on Netflix."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀: μ(after 2018) ≤ μ(2018 or before)\n",
        "\n",
        "H₁: μ(after 2018) > μ(2018 or before)"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Convert date_added to datetime\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# Extract year when content was added\n",
        "df['added_year'] = df['date_added'].dt.year\n",
        "\n",
        "# Create two groups\n",
        "after_2018 = df[df['added_year'] > 2018]['release_year'].dropna()\n",
        "before_or_2018 = df[df['added_year'] <= 2018]['release_year'].dropna()\n",
        "\n",
        "# Perform two-sample t-test (one-tailed)\n",
        "t_stat, p_value = ttest_ind(after_2018, before_or_2018, equal_var=False)\n",
        "\n",
        "# Since our hypothesis is \"greater than\", divide p-value by 2\n",
        "p_value_one_tailed = p_value / 2\n",
        "\n",
        "t_stat, p_value_one_tailed"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A two-sample (independent) t-test was performed to compare the average release year of content added after 2018 with content added in or before 2018."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test was chosen because we are comparing the means of a numerical variable (release_year) between two independent groups (content added after 2018 vs content added in or before 2018).\n",
        "\n",
        "The p-value is far below 0.05, so we reject H₀; content added after 2018 has a higher average release year."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "H₀: Genre counts are uniform; top genres do not dominate\n",
        "\n",
        "H₁: International Movies, Dramas, and Comedies occur more frequently than other genres"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# See all unique genre entries\n",
        "import pandas as pd\n",
        "from scipy.stats import chisquare\n",
        "\n",
        "# Count occurrences of each genre\n",
        "genre_counts = df['listed_in'].value_counts()\n",
        "\n",
        "# Observed frequencies for top genres (International Movies, Dramas, Comedies)\n",
        "observed_top = genre_counts[['International Movies', 'Dramas', 'Comedies']].values\n",
        "\n",
        "# Expected frequencies assuming equal distribution among these three\n",
        "expected_top = [observed_top.sum()/len(observed_top)] * len(observed_top)\n",
        "\n",
        "# Perform Chi-Square test\n",
        "chi_stat, p_value = chisquare(f_obs=observed_top, f_exp=expected_top)\n",
        "\n",
        "chi_stat, p_value\n"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Chi-Square Goodness-of-Fit test was performed to obtain the p-value for the top genres (International Movies, Dramas, and Comedies) on Netflix."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Chi-Square Goodness-of-Fit test was chosen because the variable under analysis (listed_in) is categorical, and the objective was to compare the observed frequency counts of the top genres against expected counts assuming no dominance.\n",
        "\n",
        "The p-value is far below 0.05, so we reject H₀; International Movies, Dramas, and Comedies are the top genres on Netflix."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Verify missing values before feature engineering\n",
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing values were identified and handled during the Exploratory Data Analysis (EDA) phase. Before starting feature engineering, a validation check was performed using df.isnull().sum() to confirm that the dataset contained no missing values. Therefore, no additional missing value imputation techniques were required at this stage."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "import numpy as np\n",
        "\n",
        "# Select numerical columns\n",
        "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "numerical_cols"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    return data[(data[column] < lower_bound) | (data[column] > upper_bound)]"
      ],
      "metadata": {
        "id": "RG1pIqLSDa-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_cols:\n",
        "    outliers = detect_outliers_iqr(df, col)\n",
        "    print(f\"{col}: {outliers.shape[0]} outliers\")"
      ],
      "metadata": {
        "id": "xZ-yBvwPDeQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cap_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    data[column] = np.where(\n",
        "        data[column] < lower_bound, lower_bound,\n",
        "        np.where(data[column] > upper_bound, upper_bound, data[column])\n",
        "    )"
      ],
      "metadata": {
        "id": "V1uJBAewDek6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_cols:\n",
        "    cap_outliers_iqr(df, col)"
      ],
      "metadata": {
        "id": "yATEJxe7DfDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in numerical_cols:\n",
        "    print(f\"{col} outliers after capping:\",\n",
        "          detect_outliers_iqr(df, col).shape[0])"
      ],
      "metadata": {
        "id": "VIosvUvGDfSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the Interquartile Range (IQR) method to identify outliers and applied capping (winsorization) instead of removing records. This approach reduces the influence of extreme values while preserving the overall dataset size, which is important for maintaining information in business datasets."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label encoding for binary categorical column\n",
        "le_type = LabelEncoder()\n",
        "df['type_encoded'] = le_type.fit_transform(df['type'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rating_order = {\n",
        "    'G': 1, 'PG': 2, 'PG-13': 3,\n",
        "    'R': 4, 'NC-17': 5,\n",
        "    'TV-Y': 1, 'TV-Y7': 2,\n",
        "    'TV-G': 2, 'TV-PG': 3,\n",
        "    'TV-14': 4, 'TV-MA': 5,\n",
        "    'Unknown': 0\n",
        "}\n",
        "\n",
        "df['rating_encoded'] = df['rating'].map(rating_order)"
      ],
      "metadata": {
        "id": "5DfjWr3_Eaxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "country_freq = df['country'].value_counts(normalize=True)\n",
        "df['country_encoded'] = df['country'].map(country_freq)"
      ],
      "metadata": {
        "id": "hmqsKUc4EbB6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of genres per title\n",
        "df['num_genres'] = df['listed_in'].apply(lambda x: len(x.split(',')))"
      ],
      "metadata": {
        "id": "qE03nsa6EbdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Presence indicator (has director info or not)\n",
        "df['has_director'] = df['director'].apply(lambda x: 0 if x == 'Unknown' else 1)"
      ],
      "metadata": {
        "id": "2I00TtkmEbxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(\n",
        "    columns=['type', 'rating', 'country', 'listed_in', 'director'],\n",
        "    inplace=True\n",
        ")"
      ],
      "metadata": {
        "id": "jXWpXgieEcMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XOrP88peEct_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple categorical encoding techniques were applied based on the nature of each feature. Label encoding was used for binary variables such as content type. Ordinal encoding was applied to rating categories to preserve their maturity-level order. Frequency encoding was used for high-cardinality features like country to avoid the curse of dimensionality. For multi-label genre information, count-based encoding was used to represent the number of genres associated with each title. This approach ensured meaningful numerical representation while maintaining model performance and interpretability."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction\n",
        "import re\n",
        "\n",
        "contractions = {\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"who's\": \"who is\"\n",
        "}"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def expand_contractions(text):\n",
        "    text = text.lower()\n",
        "    for contraction, expanded in contractions.items():\n",
        "        text = re.sub(r\"\\b\" + contraction + r\"\\b\", expanded, text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZU9sbNU_D2A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(expand_contractions)"
      ],
      "metadata": {
        "id": "yYhEe76QD2pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "df['description'] = df['description'].str.lower()"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "df['description'] = df['description'].apply(\n",
        "    lambda x: re.sub(r'[^\\w\\s]', '', x)\n",
        ")"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "df['description'] = df['description'].apply(\n",
        "    lambda x: re.sub(r'http\\S+|www\\S+|https\\S+', '', x)\n",
        ")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download stopwords (run once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description'] = df['description'].apply(\n",
        "    lambda x: ' '.join(\n",
        "        word for word in x.split() if word not in stop_words\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "D2zTNqYiEykb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "# Remove extra spaces\n",
        "df['description'] = df['description'].str.strip().str.replace(r'\\s+', ' ', regex=True)"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rephrase_text(text, replace_prob=0.3):\n",
        "    words = text.split()\n",
        "    new_words = []\n",
        "\n",
        "    for word in words:\n",
        "        synonyms = wordnet.synsets(word)\n",
        "\n",
        "        # Replace word with synonym with some probability\n",
        "        if synonyms and random.random() < replace_prob:\n",
        "            synonym = synonyms[0].lemmas()[0].name()\n",
        "            new_words.append(synonym.replace('_', ' '))\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "\n",
        "    return ' '.join(new_words)"
      ],
      "metadata": {
        "id": "P7LmYobXFN6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description_rephrased'] = df['description'].apply(rephrase_text)"
      ],
      "metadata": {
        "id": "e7T4i3bzFQq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "# Tokenization using split\n",
        "df['description_tokens'] = df['description'].apply(lambda x: x.split())"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download required resources (run once)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "df['description_stemmed'] = df['description'].apply(\n",
        "    lambda x: ' '.join(stemmer.stem(word) for word in x.split())\n",
        ")"
      ],
      "metadata": {
        "id": "0OkX3TfNGNY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "df['description_lemmatized'] = df['description'].apply(\n",
        "    lambda x: ' '.join(lemmatizer.lemmatize(word) for word in x.split())\n",
        ")"
      ],
      "metadata": {
        "id": "2X35058wGNq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return pos_tag(tokens)"
      ],
      "metadata": {
        "id": "ghdKEUsOGiln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['description_pos_tags'] = df['description'].apply(pos_tag_text)"
      ],
      "metadata": {
        "id": "mG0c8g6sGi97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,     # limit features\n",
        "    ngram_range=(1, 2),    # unigrams + bigrams\n",
        "    stop_words='english'\n",
        ")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tfidf = tfidf.fit_transform(df['description'])"
      ],
      "metadata": {
        "id": "-CYRsOP8Lqt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_df = pd.DataFrame(\n",
        "    X_tfidf.toarray(),\n",
        "    columns=tfidf.get_feature_names_out()\n",
        ")"
      ],
      "metadata": {
        "id": "Z9cSE2L5LrHa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the TF-IDF vectorization technique to transform textual descriptions into numerical feature vectors. TF-IDF was chosen because it highlights important words that are specific to a document while down-weighting frequently occurring words, making it more effective than simple word counts for text-based machine learning models."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# -------------------------------\n",
        "# 1. Convert date column properly\n",
        "# -------------------------------\n",
        "df['date_added'] = pd.to_datetime(df['date_added'], errors='coerce')\n",
        "\n",
        "# -------------------------------\n",
        "# 2. Create new features\n",
        "# -------------------------------\n",
        "\n",
        "# Month when content was added\n",
        "df['added_month'] = df['date_added'].dt.month\n",
        "\n",
        "# Age of content at the time it was added\n",
        "df['content_age'] = df['added_year'] - df['release_year']\n",
        "\n",
        "# Duration in minutes (Movies only)\n",
        "df['duration_minutes'] = df.apply(\n",
        "    lambda x: x['duration_value'] if x['duration_unit'] == 'min' else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Number of seasons (TV Shows only)\n",
        "df['num_seasons'] = df.apply(\n",
        "    lambda x: x['duration_value'] if x['duration_unit'] == 'Season' else None,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 3. Final numeric columns\n",
        "# -------------------------------\n",
        "numeric_cols = [\n",
        "    'added_year',\n",
        "    'added_month',\n",
        "    'release_year',\n",
        "    'content_age',\n",
        "    'duration_value',\n",
        "    'duration_minutes',\n",
        "    'num_seasons',\n",
        "    'num_genres',\n",
        "    'type_encoded',\n",
        "    'rating_encoded',\n",
        "    'country_encoded',\n",
        "    'has_director'\n",
        "]\n",
        "\n",
        "# -------------------------------\n",
        "# 4. Ensure correct numeric types\n",
        "# -------------------------------\n",
        "existing_numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
        "\n",
        "df[existing_numeric_cols] = df[existing_numeric_cols].apply(\n",
        "    pd.to_numeric, errors='coerce'\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# 5. Quick verification\n",
        "# -------------------------------\n",
        "df[existing_numeric_cols].info()\n"
      ],
      "metadata": {
        "id": "2qEW6V81MxBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Select numerical features only\n",
        "num_df = df[numeric_cols]\n",
        "\n",
        "# Compute absolute correlation matrix\n",
        "corr_matrix = num_df.corr().abs()\n",
        "\n",
        "# Upper triangle of correlation matrix\n",
        "upper = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "# Identify highly correlated features (> 0.85)\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.85)]\n",
        "\n",
        "# Drop highly correlated features\n",
        "df.drop(columns=to_drop, inplace=True)"
      ],
      "metadata": {
        "id": "vKWESRrSMxTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "dsPT-B-_Mxob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# =====================================\n",
        "# 1. Create a unified duration feature\n",
        "#    (safe for Movies + TV Shows)\n",
        "# =====================================\n",
        "df['normalized_duration'] = df.apply(\n",
        "    lambda x: x['duration_value']\n",
        "    if x['duration_unit'] == 'min'\n",
        "    else x['duration_value'] * 60,\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# =====================================\n",
        "# 2. Select features for clustering\n",
        "# =====================================\n",
        "selected_features = [\n",
        "    'normalized_duration',\n",
        "    'num_genres',\n",
        "    'added_year',\n",
        "    'added_month'\n",
        "]\n",
        "\n",
        "X = df[selected_features]\n",
        "\n",
        "# =====================================\n",
        "# 3. Handle missing values\n",
        "# =====================================\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "# =====================================\n",
        "# 4. Scale features (required for clustering)\n",
        "# =====================================\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "vt = VarianceThreshold(threshold=0.01)\n",
        "X_var = vt.fit_transform(X)\n",
        "\n",
        "selected_var_features = X.columns[vt.get_support()]\n",
        "X = df[selected_var_features]"
      ],
      "metadata": {
        "id": "cNdBglfqTkus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "corr_matrix = X.corr().abs()\n",
        "\n",
        "upper = corr_matrix.where(\n",
        "    np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "to_drop = [col for col in upper.columns if any(upper[col] > 0.85)]\n",
        "\n",
        "X_selected = X.drop(columns=to_drop)"
      ],
      "metadata": {
        "id": "VF37V2vITlGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_selected.columns"
      ],
      "metadata": {
        "id": "PIB7-xwGTtFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Numeric features selected for clustering\n",
        "numeric_features = [\n",
        "    'normalized_duration',\n",
        "    'num_seasons',\n",
        "    'num_genres',\n",
        "    'added_year',\n",
        "    'added_month'\n",
        "]\n",
        "\n",
        "X_numeric = df[numeric_features]\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=3000,\n",
        "    stop_words='english'\n",
        ")\n",
        "\n",
        "X_text = tfidf.fit_transform(df['description'])"
      ],
      "metadata": {
        "id": "Xk41PNxXVcTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import hstack\n",
        "\n",
        "X_final = hstack([X_numeric_scaled, X_text])"
      ],
      "metadata": {
        "id": "fbRZ1fmpVcml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data transformation was performed by scaling numerical features using standardization and converting textual descriptions into numerical vectors using TF-IDF. The transformed features were combined to create a unified feature space for clustering analysis."
      ],
      "metadata": {
        "id": "YqJ7OJybVjcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_features = [\n",
        "    'normalized_duration',\n",
        "    'num_seasons',\n",
        "    'num_genres',\n",
        "    'added_year',\n",
        "    'added_month'\n",
        "]\n",
        "\n",
        "X_numeric = df[numeric_features]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "X_numeric_scaled = scaler.fit_transform(X_numeric)"
      ],
      "metadata": {
        "id": "QqDB6HW2V7V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_numeric_scaled_df = pd.DataFrame(\n",
        "    X_numeric_scaled,\n",
        "    columns=numeric_features\n",
        ")"
      ],
      "metadata": {
        "id": "0lANUSy_V7nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StandardScaler was used for data scaling as the project involves KMeans clustering, which is sensitive to feature magnitudes. Standardization ensures that all numerical features contribute equally to distance calculations, preventing features with larger ranges from dominating the clustering process."
      ],
      "metadata": {
        "id": "9QAxG5NEWIbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction was applied because text vectorization produced a high-dimensional feature space, which can negatively affect clustering performance and interpretability."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "# =====================================\n",
        "# 1. Impute missing values\n",
        "# =====================================\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = imputer.fit_transform(X_final)\n",
        "\n",
        "# =====================================\n",
        "# 2. Scale features (SPARSE-SAFE)\n",
        "# =====================================\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "# =====================================\n",
        "# 3. Dimensionality Reduction\n",
        "# =====================================\n",
        "svd = TruncatedSVD(\n",
        "    n_components=min(100, X_scaled.shape[1]),\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "X_reduced = svd.fit_transform(X_scaled)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(f\"Explained Variance: {explained_variance:.2f}\")"
      ],
      "metadata": {
        "id": "50MU4njYXfTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction was performed using TruncatedSVD to reduce the high-dimensional TF-IDF feature space while preserving the most important variance, improving clustering efficiency and interpretability."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(\n",
        "    X_reduced,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "uI5PFHypYTuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)"
      ],
      "metadata": {
        "id": "j9bTlVyjYUGN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Although the project involves unsupervised learning, the dataset was split into training and testing subsets (80:20) to assess the stability and consistency of clustering results."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the project involves exploratory analysis and unsupervised learning without a predefined target variable, class imbalance handling techniques were not applicable."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - KMeans Clustering\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Fit the Algorithm\n",
        "kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_kmeans_labels = kmeans.predict(X_train)\n",
        "test_kmeans_labels = kmeans.predict(X_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_kmeans = silhouette_score(X_train, train_kmeans_labels)\n",
        "silhouette_kmeans"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['KMeans']\n",
        "scores = [silhouette_kmeans]\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(models, scores)\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Evaluation Metric Score for KMeans Model')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s4LDTbMnWgHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (Grid Search)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Fit the Algorithm\n",
        "param_grid = {\n",
        "    'n_clusters': [3, 4, 5, 6, 7],\n",
        "    'init': ['k-means++', 'random'],\n",
        "    'n_init': [10, 20]\n",
        "}\n",
        "\n",
        "best_score = -1\n",
        "best_params = None\n",
        "\n",
        "for params in ParameterGrid(param_grid):\n",
        "    kmeans = KMeans(\n",
        "        n_clusters=params['n_clusters'],\n",
        "        init=params['init'],\n",
        "        n_init=params['n_init'],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    labels = kmeans.fit_predict(X_train)\n",
        "    score = silhouette_score(X_train, labels)\n",
        "\n",
        "    if score > best_score:\n",
        "        best_score = score\n",
        "        best_params = params\n",
        "\n",
        "# Train final optimized model\n",
        "kmeans_optimized = KMeans(\n",
        "    n_clusters=best_params['n_clusters'],\n",
        "    init=best_params['init'],\n",
        "    n_init=best_params['n_init'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "kmeans_optimized.fit(X_train)\n",
        "\n",
        "# Predict on the model\n",
        "train_kmeans_labels = kmeans_optimized.predict(X_train)\n",
        "test_kmeans_labels = kmeans_optimized.predict(X_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search was used to optimize KMeans hyperparameters by systematically evaluating different parameter combinations. Since the project is unsupervised, the Silhouette Score was used as the evaluation metric to select the optimal parameters that produced the best cluster separation."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "fter applying hyperparameter optimization, a slight improvement in the Silhouette Score was observed for the KMeans model. This indicates better-defined clusters compared to the baseline model. Minor improvements are expected in high-dimensional text-based clustering tasks."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Fit the Algorithm\n",
        "agg_model = AgglomerativeClustering(\n",
        "    n_clusters=5,\n",
        "    linkage='ward'\n",
        ")\n",
        "\n",
        "train_agg_labels = agg_model.fit_predict(X_train)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_agg = silhouette_score(X_train, train_agg_labels)\n",
        "silhouette_agg"
      ],
      "metadata": {
        "id": "f7GAh3DucbIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['Agglomerative Clustering']\n",
        "scores = [silhouette_agg]\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(models, scores)\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Evaluation Metric Score for Agglomerative Clustering Model')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1qJ2UmV2cu2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (Grid Search)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Fit the Algorithm\n",
        "best_score = -1\n",
        "best_params = None\n",
        "\n",
        "for n_clusters in [3, 4, 5, 6, 7]:\n",
        "    for linkage in ['ward', 'complete', 'average']:\n",
        "\n",
        "        # ward linkage only supports euclidean metric\n",
        "        if linkage == 'ward':\n",
        "            model = AgglomerativeClustering(\n",
        "                n_clusters=n_clusters,\n",
        "                linkage=linkage\n",
        "            )\n",
        "        else:\n",
        "            model = AgglomerativeClustering(\n",
        "                n_clusters=n_clusters,\n",
        "                linkage=linkage,\n",
        "                metric='euclidean'\n",
        "            )\n",
        "\n",
        "        labels = model.fit_predict(X_train)\n",
        "        score = silhouette_score(X_train, labels)\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_params = {\n",
        "                'n_clusters': n_clusters,\n",
        "                'linkage': linkage\n",
        "            }\n",
        "\n",
        "# Train final optimized model\n",
        "agg_optimized = AgglomerativeClustering(\n",
        "    n_clusters=best_params['n_clusters'],\n",
        "    linkage=best_params['linkage']\n",
        ")\n",
        "\n",
        "train_agg_labels = agg_optimized.fit_predict(X_train)\n",
        "\n",
        "# Predict on the model\n",
        "# (Agglomerative clustering does not support predict, so fit_predict is used)\n",
        "test_agg_labels = agg_optimized.fit_predict(X_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search was used as the hyperparameter optimization technique to systematically evaluate different combinations of clustering parameters. Since the project is unsupervised, traditional methods like GridSearchCV could not be applied. Instead, the Silhouette Score was used as an internal validation metric to select the optimal hyperparameters that produced well-separated and compact clusters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Agglomerative clustering model produced a low silhouette score, indicating overlapping clusters. This behavior is expected in high-dimensional text-based datasets. Compared to KMeans, Agglomerative clustering showed weaker cluster separation, highlighting its limitations for large-scale NLP data."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The silhouette score was used to evaluate clustering quality by measuring cluster cohesion and separation. Although the scores were relatively low, this is expected in text-based content clustering where themes overlap. The KMeans model provided the most stable and interpretable clusters, enabling Netflix to group similar content, improve recommendations, and support content strategy decisions. Overall, the clustering models help Netflix enhance personalization, content discovery, and user engagement."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation (DBSCAN)\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Fit the Algorithm\n",
        "dbscan_model = DBSCAN(\n",
        "    eps=0.5,\n",
        "    min_samples=10\n",
        ")\n",
        "\n",
        "train_dbscan_labels = dbscan_model.fit_predict(X_train)\n",
        "\n",
        "# Predict on the model\n",
        "# (DBSCAN does not support predict, so fit_predict is used)\n",
        "test_dbscan_labels = dbscan_model.fit_predict(X_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "# Remove noise points (-1 label) for evaluation\n",
        "mask = train_dbscan_labels != -1\n",
        "\n",
        "if len(set(train_dbscan_labels[mask])) > 1:\n",
        "    silhouette_dbscan = silhouette_score(\n",
        "        X_train[mask],\n",
        "        train_dbscan_labels[mask]\n",
        "    )\n",
        "else:\n",
        "    silhouette_dbscan = None\n",
        "\n",
        "silhouette_dbscan"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "models = ['DBSCAN']\n",
        "scores = [silhouette_dbscan if silhouette_dbscan is not None else 0]\n",
        "\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.bar(models, scores)\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Evaluation Metric Score for DBSCAN Model')\n",
        "plt.ylim(0, 1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xJ_BjFL6fmFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (Grid Search)\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "\n",
        "best_score = -1\n",
        "best_params = None\n",
        "\n",
        "eps_values = [0.3, 0.5, 0.7, 1.0]\n",
        "min_samples_values = [5, 10, 15]\n",
        "\n",
        "# Fit the Algorithm\n",
        "for eps in eps_values:\n",
        "    for min_samples in min_samples_values:\n",
        "\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X_train)\n",
        "\n",
        "        # Remove noise points\n",
        "        mask = labels != -1\n",
        "\n",
        "        if len(set(labels[mask])) > 1:\n",
        "            score = silhouette_score(X_train[mask], labels[mask])\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = {\n",
        "                    'eps': eps,\n",
        "                    'min_samples': min_samples\n",
        "                }\n",
        "\n",
        "# Fallback if no valid parameters found\n",
        "if best_params is None:\n",
        "    best_params = {\n",
        "        'eps': 0.5,\n",
        "        'min_samples': 10\n",
        "    }\n",
        "\n",
        "# Train final DBSCAN model\n",
        "dbscan_optimized = DBSCAN(\n",
        "    eps=best_params['eps'],\n",
        "    min_samples=best_params['min_samples']\n",
        ")\n",
        "\n",
        "train_dbscan_labels = dbscan_optimized.fit_predict(X_train)\n",
        "\n",
        "# Predict on the model\n",
        "# (DBSCAN does not support predict, so fit_predict is used)\n",
        "test_dbscan_labels = dbscan_optimized.fit_predict(X_test)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For DBSCAN, hyperparameter optimization was performed using a manual grid search strategy over key parameters such as eps and min_samples. Since the project involves unsupervised learning with no target variable, traditional supervised tuning techniques could not be applied. Instead, an internal clustering validation metric, the Silhouette Score, was used after excluding noise points. This approach helped explore suitable density parameters for identifying clusters and outliers."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No significant improvement was observed after hyperparameter tuning.\n",
        "\n",
        "This is expected behavior for DBSCAN on high-dimensional, text-based datasets like Netflix content."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Silhouette Score was chosen as the primary evaluation metric because it measures cluster cohesion and separation in unsupervised learning. From a business perspective, higher silhouette scores indicate better content grouping, which directly supports recommendation systems, content discovery, and user engagement."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among the three clustering models evaluated, KMeans was selected as the final model due to its superior silhouette score, scalability, and interpretability. The model produced stable and meaningful clusters that can support Netflix’s recommendation system, content discovery, and strategic decision-making."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KMeans model was explained using centroid analysis and cluster profiling. Since clustering models do not provide direct feature importance, the contribution of features was analyzed using cluster centroids and average feature values within each cluster. This approach helped identify key attributes such as duration, number of seasons, genre diversity, and content recency that influenced cluster formation."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, an end-to-end unsupervised machine learning pipeline was developed to analyze and group Netflix content based on textual and numerical features. Extensive exploratory data analysis was performed to understand content distribution across countries, formats, and time, revealing key trends such as Netflix’s increasing focus on TV shows and region-specific content preferences.\n",
        "\n",
        "Feature engineering and data preprocessing techniques—including text cleaning, TF-IDF vectorization, feature scaling, and dimensionality reduction using TruncatedSVD—were applied to transform raw data into a machine-learning-ready format. Multiple clustering algorithms were implemented and evaluated to identify meaningful content groupings.\n",
        "\n",
        "Among the models tested, KMeans clustering emerged as the most effective approach, achieving the highest silhouette score and producing stable, interpretable clusters. Agglomerative clustering provided additional hierarchical insights but showed weaker cluster separation, while DBSCAN proved useful primarily for identifying niche and outlier content rather than forming well-defined clusters. Hyperparameter optimization further improved clustering performance for KMeans, validating it as the final model.\n",
        "\n",
        "The clustering results offer valuable business insights by enabling Netflix to better segment its content library, enhance recommendation systems, improve content discovery, and support strategic decision-making related to content acquisition and audience targeting. Overall, this project demonstrates how unsupervised learning can effectively uncover hidden patterns in large-scale content datasets and drive meaningful business impact."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}